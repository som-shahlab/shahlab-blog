<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://som-shahlab.github.io/shahlab-website/feed.xml" rel="self" type="application/atom+xml"/><link href="https://som-shahlab.github.io/shahlab-website/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-08T02:57:36+00:00</updated><id>https://som-shahlab.github.io/shahlab-website/feed.xml</id><title type="html">Shah Lab</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Human AI Collaboration: Literature Review</title><link href="https://som-shahlab.github.io/shahlab-website/blog/2024/Human-AI-Collab/" rel="alternate" type="text/html" title="Human AI Collaboration: Literature Review"/><published>2024-07-12T00:00:00+00:00</published><updated>2024-07-12T00:00:00+00:00</updated><id>https://som-shahlab.github.io/shahlab-website/blog/2024/Human-AI-Collab</id><content type="html" xml:base="https://som-shahlab.github.io/shahlab-website/blog/2024/Human-AI-Collab/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>We have seen AI tools assisting programmers like <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">Code Llama</a>, AI tools assisting drivers with self-driving cars, but can AI tools assist clinicians in making medical decisions?</p> <p>To some this might seem like a bad idea ‚Äì what if AI makes mistakes? And will AI replace physicians? This blog post will show where AI seems to help clinicians, where it falls short, and where this field of research is going. We will explore cases where AI and clinicians, when working together in diagnosing, can provide superior performance; we will explore cases where AI was used to monitor patients alongside physicians; and we will explore cases where this AI-assistance translated into not just better accuracy, but better real-world clinical outcomes.</p> <h1 id="review-of-literature">Review of Literature</h1> <p>This paper selection primarily focuses on journal publications, specifically on studies involving the use of an AI model by clinicians as part of their decision making. This was done using PubMed and the ‚Äúrelated articles‚Äù feature. It is important to note that methods in these papers primarily involve discriminative rather than generative AI (with the exception of the study evaluating ChatGPT &amp; clinicians)<d-cite key="Goh2023-vb"></d-cite>.</p> <p>Information on these studies are compiled into the table below. Additional information for these studies are also included at the end of the blog post <strong>(please click on ‚Äúinfo‚Äù in the title column of the table to jump to that section)</strong>. The results of these papers come in three categories:</p> <ul> <li> <p>‚úÖ A green checkmark indicates results where Human-AI collaboration derived superior results.</p> </li> <li> <p>üü® A yellow box indicates that <strong>Human-AI collaboration generally provided superior results</strong> but the researchers found <strong>cases where Human-AI collaboration does not help.</strong></p> </li> <li> <p>‚ùå A red cross indicates thart Human-AI collaboration did not provide any benefits.</p> </li> </ul> <table class="l-page"> <thead> <tr> <th>Title</th> <th>Research Question</th> <th>Study Scale</th> <th>Study Design</th> <th>Did Human-AI Collaboration Work?</th> </tr> </thead> <tbody> <tr> <td>Interaction between clinicians and artificial intelligence to detect fetal atrioventricular septal defects on ultrasound: how can we optimize collaborative performance? <a href="#interaction-between-clinicians-and-artificial-intelligence-to-detect-fetal-atrioventricular-septal-defects-on-ultrasound-how-can-we-optimize-collaborative-performance">(info)</a></td> <td>Can AI assistance (including confidence and explanations) help clinicians better detect fetal atrioventricular sepal defects?</td> <td>10 Clinicians; 2000 fetal four chamber images</td> <td>Ten clinicians reviewed 2000 fetal four-chamber images in various conditions to identify normal images and those with AVSD, using different types of AI assistance (with and without classification, confidence, and explanations)</td> <td>üü® Across all kinds of clinicians, human and AI collaboration performed better than just one of the two (only human predictions or only AI predictions), but explanations or confidence didn‚Äôt compensate for incorrect model predictions.</td> </tr> <tr> <td>Measuring the Impact of AI in the Diagnosis of Hospitalized Patients. <a href="#measuring-the-impact-of-ai-in-the-diagnosis-of-hospitalized-patients">(info)</a></td> <td>Do image-based explanations reduce the negative effects of influence from systematically biased models?</td> <td>457 Clinicians</td> <td>Each clinician assessed 9 clinical vignettes for the likelihood of pneumonia, heart failure, or COPD, with varying AI prediction types (standard or systematically biased) and a peer consultation designed to provide upper-bound diagnostic accuracy.</td> <td>üü® Standard AI predictions helped clinicians, and standard AI predictions with explanations further increased accuracy. Participant diagnostic accuracy was highest with an always-correct clinical consultation. Systematically biased AI predictions decreased clinician accuracy regardless of whether explanations were provided.</td> </tr> <tr> <td>AI-enabled electrocardiography alert intervention and all-cause mortality. <a href="#ai-enabled-electrocardiography-alert-intervention-and-all-cause-mortality">(info)</a></td> <td>Does a warning message to the treating physician generated by an AI model (identifying patients with high risk) prompt management decisions leading to a decrease of deaths?</td> <td>39 Clinicians; 15,965 Patients</td> <td>The study evaluated if the AI-ECG model could identify deteriorating patients and reduce the need for additional ECGs by comparing mortality rates between an intervention group receiving AI alerts and a control group monitored by physicians alone.</td> <td>‚úÖ The study showed a significant decrease in mortality with AI-ECG assistance compared to the control group, particularly in high-risk cases, and suggested that the AI model‚Äôs novelty and manageable alert frequency helped maintain quality of care and reduce mortality through increased physician attention.</td> </tr> <tr> <td>Artificial intelligence-assisted diagnosis of congenital heart disease and associated pulmonary arterial hypertension from chest radiographs. <a href="#artificial-intelligence-assisted-diagnosis-of-congenital-heart-disease-and-associated-pulmonary-arterial-hypertension-from-chest-radiographs">(info)</a></td> <td>Can AI predictions help radiologists in diagnosing congenital heart disease (CHD) and associated pulmonary arterial hypertension (PAH-CHD)?</td> <td>5 Clinicians; 495 CXR images</td> <td>The study was a multireader multicase randomized crossover study, where clinicians reviewed each chest x-ray (CXR) for CHD and PAH-CHD twice (with a 1-month washout period), once with an AI prediction and once without.</td> <td>‚úÖ AI predictions improved clinician accuracy for both CHD and PAH-CHD.</td> </tr> <tr> <td>Use of Voice-Based Conversational Artificial Intelligence for Basal Insulin Prescription Management Among Patients With Type 2 Diabetes: A Randomized Clinical Trial. <a href="#use-of-voice-based-conversational-artificial-intelligence-for-basal-insulin-prescription-management-among-patients-with-type-2-diabetes-a-randomized-clinical-trial">(info)</a></td> <td>Does a voice-based interface help in providing improved ‚Äútime to optimal insulin dosing, insulin adherence, and glycemic control‚Äù?</td> <td>32 Patients</td> <td>The intervention group had an Amazon smart speaker which gave them information for insulin management, while the control group was instructed to fill out an online blood glucose and insulin log daily for the duration of the trial.</td> <td>‚úÖ PAID-5 scores were lower for the intervention group compared to the control. PAID-5 is a qualitative survey measurement for diabetes where lower scores are more favorable.</td> </tr> <tr> <td>Conventional Versus Artificial Intelligence-Assisted Interpretation of Chest Radiographs in Patients With Acute Respiratory Symptoms in Emergency Department: A Pragmatic Randomized Clinical Trial. <a href="#conventional-versus-artificial-intelligence-assisted-interpretation-of-chest-radiographs-in-patients-with-acute-respiratory-symptoms-in-emergency-department-a-pragmatic-randomized-clinical-trial">(info)</a></td> <td>Does AI-assistance (predictions) help radiologists interpret chest radiographs in patients with acute respiratory symptoms in ED?</td> <td>35,756 Patients</td> <td>Patients were put into an intervention and a control group in a ratio of 1:1. The intervention group had clinicians review the chest radiograph with an AI model prediction accompanied, while the control group did not.</td> <td>‚ùå There was no association between using AI predictions and not using predictions with respect to clinician performance</td> </tr> <tr> <td>ChatGPT Influence on Medical Decision-Making, Bias, and Equity: A Randomized Study of Clinicians Evaluating Clinical Vignettes. <a href="#chatgpt-influence-on-medical-decision-making-bias-and-equity-a-randomized-study-of-clinicians-evaluating-clinical-vignettes">(info)</a></td> <td>Can AI positively influence physicians in their clinical decision making without introducing or exacerbating demographic bias?</td> <td>50 Clinicians</td> <td>Participants reviewed a clinical vignette video (randomly chosen whether it featured a white male or a black female), and could then change their answers after reviewing suggestions from ChatGPT+.</td> <td>‚úÖ In both cases (vignettes featuring a white male and a black female), accuracy by the clinicians increased by 18% after they reviewed suggestions from ChatGPT+.</td> </tr> <tr> <td>Artificial intelligence suppression as a strategy to mitigate artificial intelligence automation bias. <a href="#artificial-intelligence-suppression-as-a-strategy-to-mitigate-artificial-intelligence-automation-bias">(info)</a></td> <td>Can AI-induced automation bias be mitigated using AI?</td> <td>40 Clinicians; 200 cases</td> <td>Clinicians were assigned to either the AI-assisted group or the AI-unassisted group. After a washout period of 14 days, clinicians switched to the other group and diagnosed same 200 MRIs.</td> <td>üü® Application of AI significantly increased the clinicians‚Äô accuracy, but 45.5% of the total mistakes in the AI-assisted round were associated with clinicians being misleaded by AI predictions, though all clinicians denied that AI predictions misled them in any way.</td> </tr> </tbody> </table> <h1 id="conclusion">Conclusion</h1> <p>These are our takeaways from this review:</p> <h5 id="human-ai-collaboration-generally-works-as-long-as-ai-is-accurate">Human-AI collaboration generally works, as long as AI is accurate</h5> <p>With the exception of one study, all experiments showed that providing AI predictions to clinicians offered better accuracy compared to not providing AI predictions. However, most studies also displayed how incorrect AI predictions can adversely influence clinicians, a case of automation bias.</p> <p class="l-gutter">Automation bias is the tendency to over-rely on automation.<d-cite key="Goddard2012-wf"></d-cite></p> <h5 id="ai-models-can-help-clinicians-even-if-ai-models-arent-perfect">AI models can help clinicians even if AI models aren‚Äôt perfect</h5> <p>Even though AI models may not have perfect accuracy, Human-AI collaboration has shown to provide better performance than both</p> <ul> <li>evaluation by clinicians without AI-assistance; and</li> <li>AI predictions without clinician influence.</li> </ul> <p>Studies like <em>Interaction between clinicians and artificial intelligence to detect fetal atrioventricular septal defects on ultrasound</em><d-cite key="Day2024-wh"></d-cite> and <em>ChatGPT Influence on Medical Decision-Making, Bias, and Equity</em><d-cite key="Goh2023-vb"></d-cite> exhibited this phenomenon.</p> <h5 id="providing-ai-confidence-or-explanations-does-not-compensate-for-incorrect-predictions">Providing AI confidence or explanations does not compensate for incorrect predictions</h5> <p>Studies like <em>Interaction between clinicians and artificial intelligence to detect fetal atrioventricular septal defects on ultrasound</em><d-cite key="Day2024-wh"></d-cite> and <em>Measuring the Impact of AI in the Diagnosis of Hospitalized Patients</em><d-cite key="Jabbour2023-rd"></d-cite> show that using explanations or confidence levels to assist an AI prediction doesn‚Äôt substantially prevent clinicians from recognizing incorrect AI predictions, resulting in worse performance from the Human-AI collaboration.</p> <h1 id="future-work">Future Work</h1> <h5 id="beyond-retrospective-studies">Beyond retrospective studies</h5> <p>So far, a lot of these studies compared Human-AI collaboration with retrospective data. This means clinicians reviewed data from patients who have already been treated/attended to. This leads these studies to not be the best representation of how humans and AI algorithms can work together in the real-world, since the collaboration isn‚Äôt happening in real time as patients are getting treated. Some of these papers did indeed differ:</p> <ul> <li> <p><em>AI-enabled electrocardiography alert intervention and all-cause mortality</em><d-cite key="Lin2024-oo"></d-cite>explored this real-time Human-AI interaction and provided insights on issues like alert-fatigue, which cannot necessarily be assessed in retrospective data studies.</p> </li> <li> <p><em>Conventional Versus Artificial Intelligence-Assisted Interpretation of Chest Radiographs in Patients With Acute Respiratory Symptoms in Emergency Department</em><d-cite key="Hwang2023-lv"></d-cite> also featured this kind of research, with an AI prediction being provided immediately to the radiologists while in the emergency department</p> </li> <li> <p><em>Use of Voice-Based Conversational Artificial Intelligence for Basal Insulin Prescription Management Among Patients With Type 2 Diabetes</em><d-cite key="Nayak2023-em"></d-cite> featured a real-time Human-AI interaction environment, but not primarily between AI and clinicians (but rather studied AI-patient interaction).</p> </li> </ul> <p>However, there is a need for <em>more</em> studies that evaluate Human-AI collaboration in situations when the ‚Äúground truth‚Äù isn‚Äôt immediately available, where decisions beyond diagnosis must be made with purely the clinician and AI‚Äôs judgement.</p> <h5 id="iterative-interaction-with-generative-ai">Iterative interaction with generative AI</h5> <p>Additionally, most of these studies focus on a single interaction between the AI model output and the clinician for a given case. Though most papers that were reviewed do not use generative AI (which calls for iterative interaction), <em>ChatGPT Influence on Medical Decision-Making, Bias, and Equity</em><d-cite key="Goh2023-vb"></d-cite> explores an iterative interaction, where a clinician views the case initially and then proceeds to consult an AI model for further analysis. However, there is a need for more research in generative AI-human collaboration when it comes to clinical settings, including finding methods to evaluate automation bias in these Human-AI collaboration methods.</p> <hr/> <h1 id="more-info-on-the-papers">More Info on the Papers</h1> <p>This section includes additional information on the papers that were reviewed, including more details on how the study was conducted, who the clinicans were, and the kinds of AI models used for the study.</p> <h2 id="interaction-between-clinicians-and-artificial-intelligence-to-detect-fetal-atrioventricular-septal-defects-on-ultrasound-how-can-we-optimize-collaborative-performance">Interaction between clinicians and artificial intelligence to detect fetal atrioventricular septal defects on ultrasound: how can we optimize collaborative performance?<d-cite key="Day2024-wh"></d-cite></h2> <h5 id="question-can-ai-assistance-including-confidence-and-explanations-help-clinicians-better-detect-fetal-atrioventricular-sepal-defects">Question: Can AI assistance (including confidence and explanations) help clinicians better detect fetal atrioventricular sepal defects?</h5> <p>This observational study with retrospectively collected data assessed the performance of clinicians, interpreting fetal four-chamber images. The study evaluated how AI predictions and associated confidence &amp; explanations affect clinicans‚Äô diagnosis of atrioventricular sepal defects (AVSD).</p> <h3 id="study-details">Study Details</h3> <p><strong>Study Type:</strong> Observational study with retrospectively collected data</p> <p>Each clinician was shown 2000 fetal four chamber images in random order, of which 1000 were normal, and the other 1000 had AVSD. The dataset contained a total of 500 images, and each image was shown in 4 conditions:</p> <ul> <li>Image alone without AI output</li> <li>Image with binary AI classification</li> <li>Image with AI model confidence</li> <li>Image with Grad-CAM</li> </ul> <h3 id="who-were-the-clinicians">Who were the clinicians?</h3> <p>The study consisted of 10 clinicians:</p> <ul> <li>2 consultant fetal cardiologists</li> <li>3 trainees in pediatric cardiology</li> <li>5 fetal cardiac sonographers</li> </ul> <h3 id="model-details">Model Details</h3> <p>The model used a ResNet50 model architecture. Temperature scaling of model prediction probability on validation set was conducted.</p> <h4 id="dataset">Dataset</h4> <p>The training data was a cohort of 121,130 cardiac four chamber images extracted from 173 ultrasound scan videos (98 normal, 75 with AVSD).</p> <h4 id="explainability-method-and-uncertainty-estimation">Explainability Method and Uncertainty Estimation</h4> <p>Grad-CAM was used for providing explanations. The model confidence was also provided to clinicians.</p> <h3 id="does-human--ai-collaboration-work">Does Human + AI collaboration work?</h3> <p>Across all kinds of clinicians, human and AI collaboration performed better than just one of the two (only human predictions or only AI predictions).</p> <h4 id="what-happens-when-ai-predicts-incorrectly">What happens when AI predicts incorrectly?</h4> <p>Incorrect model outputs worsened clinician performance, which is expected as part of automation bias. Additional information was also provided to reduce negative effects of bad predictions (Confidence &amp; GradCAM), but these actually worsened overall performance.</p> <p><img src="/shahlab-website//assets/img/day_et_al_humanAICollab.png" alt="Img"/> <em>Image from paper</em></p> <h2 id="measuring-the-impact-of-ai-in-the-diagnosis-of-hospitalized-patients">Measuring the Impact of AI in the Diagnosis of Hospitalized Patients.<d-cite key="Jabbour2023-rd"></d-cite></h2> <h5 id="question-do-image-based-explanations-reduce-the-negative-effects-of-influence-from-systematically-biased-models">Question: Do image-based explanations reduce the negative effects of influence from systematically biased models?</h5> <h3 id="study-details-1">Study Details</h3> <p><strong>Study Type:</strong> Randomized Clinical Vignette Survey Study</p> <p>The study contained 45 clinical vignettes. Each clinician viewed 9 vignettes. The vignettes were in the following order:</p> <ul> <li>Vignettes 1 &amp; 2: no AI prediction</li> <li>Vignettes 3-8: <ul> <li>Half shown with prediction from <strong>one of three</strong> standard AI models</li> <li>Other half shown with <strong>corresponding</strong> systematically biased AI model prediction</li> <li>Which vignettes are biased are randomly determined</li> </ul> </li> <li>Vignette 9 <ul> <li>Clinical consultation (peer consultation provided to the clinician)</li> <li>By design, the clinical consultation was always correct, intended to provide an upper-bound clinican diagnostic accuracy</li> </ul> </li> </ul> <p>After each vignette, the clinicians were asked to separately assess how likely (from a scale from 1 to 100) pneumonia, heart failure, or COPD was contributing to the patient‚Äôs respiratory failure.</p> <p><img src="/shahlab-website//assets/img/jabbour_et_al_humanAICollab.png" alt="Img"/> <em>Image from paper</em></p> <h3 id="who-were-the-clinicians-1">Who were the clinicians?</h3> <p>The study ‚Äúrecruited hospitalist physicians, nurse practitioners, and physician assistants who commonly care for patients with acute respiratory failure‚Äù.</p> <h3 id="model-details-1">Model Details</h3> <h4 id="standard-models">Standard models</h4> <p>Three standards AI models were used:</p> <ul> <li>One detecting pneumonia</li> <li>One detecting heart failure</li> <li>One detecting COPD</li> </ul> <h4 id="biased-models">Biased models</h4> <p>Each of the models had a corresponding systematically biased counterpart:</p> <ul> <li>one predicted high likelihood of <em>pneumonia</em> for a patient who was <em>80 years old or older</em></li> <li>one predicted high likelihood of <em>heart failure</em> for a patient who has a <em>BMI greater than or equal to 30</em></li> <li>and one predicted high likelihood of <em>COPD</em> for a patient if <em>there was a blur applied on the radiograph</em></li> </ul> <h4 id="explainability-method">Explainability method</h4> <p>The models used saliency maps to display the most relevant parts of the input.</p> <h3 id="does-human--ai-collaboration-work-1">Does Human + AI collaboration work?</h3> <h4 id="standard-ai-models">Standard AI models</h4> <p>Participant‚Äôs baseline diagnostic accuracy <strong>without model input</strong> was 73.0% (95% CI, 68.3%-77.8%) for the 3 diagnoses. When clinicians were provided standard AI predictions (<strong>without explanations</strong>), participant diagnostic accuracy for each disease <strong>increased</strong> to 75.9% (95% CI, 71.3%-80.5%). Providing standard AI predictions <strong>with explanations</strong> increased accuracy to 77.5% (95% CI, 73.0%-82.0%), <strong>higher than just that when predictions alone were provided</strong>. Participant diagnostic accuracy was highest, with 81.1% (95% CI, 76.9%-85.4%), when physicians were provided a clinical consultation with perfect accuracy.</p> <h4 id="systematically-biased-ai-models">Systematically biased AI models</h4> <p>Systematically biased AI predictions <strong>without explanations</strong> caused a <strong>decrease</strong> in participant diagnostic accuracy to 61.7% (95% CI, 55.3%-68.2%), a decrease of 11.3 percentage points (95% CI, 7.2-15.5; P &lt; .001) from the baseline accuracy. Biased AI predictions <strong>with explanations</strong> <strong>did not help</strong>, with a similar 64.0% (95% CI, 57.6%-70.3%) accuracy, which is a decrease of 9.1 percentage points (95% CI, 4.9-13.2; P&lt;.001) from the baseline accuracy.</p> <h4 id="other-observations">Other observations</h4> <p>Even clinicians when were provided perfectly accurate recommendations in the clinical consultation vignettes, they had only 81.1% diagnostic accuracy. This might indicate that clinicians have difficulty in recognizing perfectly accurate reccomendations, suggesting that ‚Äúas AI models improve, there may be an <strong>upper bound on collaborative performance</strong> between AI models and clinicians for difficult diagnostic tasks.‚Äù</p> <h2 id="ai-enabled-electrocardiography-alert-intervention-and-all-cause-mortality">AI-enabled electrocardiography alert intervention and all-cause mortality.<d-cite key="Lin2024-oo"></d-cite></h2> <h5 id="question-does-a-warning-message-to-the-treating-physician-generated-by-an-ai-model-identifying-patients-with-high-risk-prompt-management-decisions-leading-to-a-decrease-of-deaths">Question: Does a warning message to the treating physician generated by an AI model (identifying patients with high risk) prompt management decisions leading to a decrease of deaths?</h5> <p>This study evaluated how effective an AI model can identify deterioirating patients (using ECGs) and how such predictions (which triggered warnings to physicians), affected mortality.</p> <h3 id="study-details-2">Study Details</h3> <p><strong>Study Type:</strong> Pragmatic randomized clinical trial</p> <p>This study utilized an AI model (AI-ECG) in order to identify deterioirating patients whose clinical conditions are possible reversible. This was done to evaluate whether such as system could replace the need to conduct aditional ECGs and alter routine daily practice. The control group had physicians monitoring patients by themselves, while the intervention group had an AI-based model provide alerts to physicians. The study evaluated whether the cumulative proportion of death differed between the two groups. 8,001 and 7,964 patients in the intervention and control groups, respectively, were analyzed.</p> <h3 id="who-were-the-clinicians-2">Who were the clinicians?</h3> <p>The study consisted of 39 physicians, of which the mean age was 45.6 ¬± 6.2 years and 97.4% were male.</p> <h3 id="model-details-2">Model Details</h3> <p>The model was a convolutional neural network incorporating residual and attention modules. The model‚Äôs output was transformed into a percentile score based on a hospital-based population, from the original range of negative to positive infinity (patients below 75th percentile were generally considered to have an extremely low risk of death). More details about the model are outlined in <a href="https://doi.org/10.1177/20552076231187247">this publication</a>.</p> <h4 id="dataset-1">Dataset</h4> <p>The model was trained on more than 450,000 ECGs, with the primarily label being all-cause mortality. The model used survival data and censored events as input.</p> <h3 id="model-results">Model Results</h3> <p>The AI-ECG model outperformed all baseline characteristics in predicting 90-day mortality risk: AUC = 0.886.</p> <h3 id="does-human--ai-collaboration-work-2">Does Human + AI collaboration work?</h3> <p>The study indicated a decreased mortality rate when AI assistance was used versus the control: ‚Äúthe cumulative proportion of death within 90 days was significantly different between the groups (3.6% in the intervention group versus 4.3% in the control group, HR = 0.83 and 95% CI = 0.70‚Äì0.99, P = 0.040)‚Äù. This difference was primarily due to the high-risk cases being identified by the AI-ECG, with P for the intervention group √ó risk group interaction = 0.026, indicating that the AI-ECG is more effective for identifying high-risk than low-risk patients, though that is consistent with an rapid response system. The active warning message from the AI-ECG in intervention group significantly reduced mortality risk from 23.0% to 16.0% (HR = 0.69 and 95% CI = 0.53‚Äì0.90, P = 0.006). However, the providing of an opportunity to review the AI-ECG reports only provided limited help in the AI-defined low-risk population (HR = 0.97 and 95% CI = 0.77‚Äì1.22, P = 0.777).‚Äù</p> <p>One potential explanation for the reduced alert fatigue from physicians using the AI model is could be because of the novelty of the AI model. In the study, the 39 physicians only received 709 alerts in 4.5 months, which made physicians more willing to order more intensive care. Nevertheless, thr authors anticipate that even given AI-ECG results are accessible in real time for all patients (where they expect fewer than 10 alerts per month for each physician), the burden remains within an acceptable range for maintaining quality of care. Additionally, though physicians were unaware of what specific actions they should perform given the alert, the increased attention induced by the alert could have reduced mortality.</p> <h2 id="artificial-intelligence-assisted-diagnosis-of-congenital-heart-disease-and-associated-pulmonary-arterial-hypertension-from-chest-radiographs">Artificial intelligence-assisted diagnosis of congenital heart disease and associated pulmonary arterial hypertension from chest radiographs.<d-cite key="Han2024-tq"></d-cite></h2> <h5 id="question-can-ai-predictions-help-radiologists-in-diagnosing-chd-and-pah-chd">Question: Can AI predictions help radiologists in diagnosing CHD and PAH-CHD?</h5> <p>This study explores the usage of AI to help radiologists in the identification of congenital heart disease (CHD) and pulmonary associated arterial hypertension (PAH-CHD) from chest radiographs. The study compares multiple AI model architectures, incorporates explainability through the use of class activation maps (CAMs), and compares these model performances to those of radiologists &amp; studies the performance of collaboration between those models and radiologists.</p> <h3 id="study-details-3">Study Details</h3> <p><strong>Study Type:</strong> Multireader multicase (MRMC) randomized crossover study</p> <p>This study can be split into three parts:</p> <ol> <li>comparing AI models and seeing whether they can accurately identify CHD &amp; PAH-CHD</li> <li>evaluating model performance and comparing it to clinicians (done on a balanced dataset)</li> <li>evaluating model collaboration with clinicians.</li> </ol> <p>Radiologists evaluated CXRs for classical signs of CHD. Radiologists were presented with 330 CXRs for CHD and 165 CXRs for PAH-CHD diagnosis. These images were presented in a randomized order. The first time, 50% images accompanied with AI-based classification (clinicians were made aware that AI not always right), and the rest were not. Radiologists provided their classification. The second time (which proceeded after a 1-month washout period), 50% of the other images were with AI-based classification, while the rest were not. This meant that diagnoses made by the radiologists were collected for each image with and without AI prediction assisting the clinician.</p> <h3 id="who-were-the-clinicians-3">Who were the clinicians?</h3> <p>5 radiologists:</p> <ul> <li>3 junior radiologists with over 3 years of experience</li> <li>2 senior radiologists with over 8 years of experience</li> </ul> <h3 id="model-details-3">Model Details</h3> <p>4 models were evaluated:</p> <ol> <li>ResNet18</li> <li>Densnet121</li> <li>MobileNetv2-120</li> <li>MobileViT-S</li> </ol> <h4 id="explainability-method-1">Explainability Method</h4> <p>The study used class activation maps (CAMs) to identify most salient features to the AI models‚Äô classification.</p> <h3 id="model-results-1">Model Results</h3> <h4 id="chd">CHD</h4> <p>ResNet18 pretrained with the ImageNet database yielded</p> <ul> <li>the highest sensitivity of 0.970,</li> <li>highest specificity of 0.982</li> <li>excellent AUC of 0.948.</li> </ul> <h4 id="pah-chd">PAH-CHD</h4> <p>PAH-CHD, the AI model achieved an AUC of 0.778 (sensitivity, 0.632; specificity, 0.925; accuracy, 0.891; F1 score, 0.571). Salient features for the identification of PAH-CHD were mainly in the projection area of the pulmonary artery segments.</p> <h3 id="model-vs-human-performance">Model vs. Human performance</h3> <h4 id="chd-1">CHD</h4> <p>Junior radiologists achieved AUCs of 0.670‚Äì0.809, and senior radiologists achieved AUCs of 0.803‚Äì0.858, which were significantly lower than those of the AI model (for CHD).</p> <h4 id="pah-chd-1">PAH-CHD</h4> <p>AI model was significantly better than the performance of one junior (0.557; P = 0.008) radiologist and comparable to that of two junior and two senior (0.610‚Äì0.688; all P &gt; 0.05) radiologists.</p> <h3 id="does-human--ai-collaboration-work-3">Does Human + AI collaboration work?</h3> <h4 id="chd-2">CHD</h4> <p>With AI assistance, all radiologists exhibited an increase in mean ¬± standard deviation (SD) sensitivity from 0.728 ¬± 0.194 to 0.861 ¬± 0.087 ($\delta$ sensitivity + 0.132, 95 % CI: -0.039‚Äì0.303; P = 0.099) for CHD diagnosis. The radiologists exhibited a significant increase in the mean ¬± SD specificity from 0.832 ¬± 0.107 to 0.891 ¬± 0.093 ($\delta$ specificity + 0.059, 95 % CI: 0.013‚Äì0.106; P = 0.023).</p> <h4 id="pah-chd-2">PAH-CHD</h4> <p>With AI assistance, the mean ¬± SD sensitivity of all radiologists increased from 0.537 ¬± 0.131 to 0.674 ¬± 0.068. No significant difference was observed between radiologists without and with AI assistance in terms of their mean ¬± SD specificity</p> <h3 id="other-relevant-information">Other relevant information</h3> <p>In order to decrease the missed diagnosis of CHD, the researchers used the AI model with the <strong>highest sensitivity value instead of AUC</strong>. This sensitivity advantage exhibited by the AI model suggests that it could help identify as many CHD patients as possible, allowing therapy management and potentially improve clinical outcomes.</p> <p>The mean specificity of radiologists who used AI assistance <strong>was not significantly different but even slightly lower</strong> compared to that of radiologists who did not use AI for diagnosing PAH-CHD. This decrease in radiologists‚Äô ability to identify negative cases (with AI assistance) could be due to radiologists choosing to diagnose a case as positive when they alone diagnose negative results but the AI prediction is positive.</p> <h2 id="use-of-voice-based-conversational-artificial-intelligence-for-basal-insulin-prescription-management-among-patients-with-type-2-diabetes-a-randomized-clinical-trial">Use of Voice-Based Conversational Artificial Intelligence for Basal Insulin Prescription Management Among Patients With Type 2 Diabetes: A Randomized Clinical Trial.<d-cite key="Nayak2023-em"></d-cite></h2> <h5 id="question-does-a-voice-based-interface-help-in-providing-improved-time-to-optimal-insulin-dosing-insulin-adherence-and-glycemic-control">Question: Does a voice-based interface help in providing improved ‚Äútime to optimal insulin dosing, insulin adherence, and glycemic control‚Äù?</h5> <p>This study explores the field of Human-AI collaboration in the clinical setting from a different perspective: rather than having AI models collaborate with clinicians, the AI model in the study assists patients directly through a voice interface. The study assesses the efficacy of a voice-based AI interface for Basal Insulin Prescription Management for patients with Type 2 diabetes.</p> <h3 id="study-details-4">Study Details</h3> <p><strong>Study Type:</strong> Randomized clinical trial</p> <h4 id="participants">Participants</h4> <p>Adults with type 2 diabetes who required initiation or adjustment of once-daily basal insulin participated in the study. The exclusion criteria was any factores that were technical barriers to administering this insulin at home. Patients were also English speaking.</p> <p>Participants were randomly assigned in a 1:1 ratio to receive insulin management either from the AI model or standard of care. Randomization was computer-generated and stratified by age &amp; gender. The study used a permuted block design (random block sizes of 2 and 4).</p> <h4 id="intervention-group">Intervention Group</h4> <p>Each participant sent Amazon smart speaker with the AI. Clinician preset the device with protocol whose parameters included a</p> <ul> <li>starting insulin dose</li> <li>goal fasting blood glucose (FBG) level range</li> <li>insulin titration instructions</li> </ul> <p>Participants were instructed to check in with the AI daily by using the phrase ‚ÄúAlexa, check in with clinical trial.‚Äù</p> <h4 id="control-group">Control Group</h4> <p>Each participant was instructed to fill out an online blood glucose and insulin log daily for the duration of the trial. Each participant was also sent Amazon smart speaker, but it just gave daily reminders to complete their log.</p> <h3 id="model-details-4">Model Details</h3> <p>The model was rule-based. The rules were developed from titration algorithms by the American Association of Clinical Endocrinologists and the American College of Endocrinology. These rules also had included emergency protocols to handle hypoglycemia and hyperglycemia.</p> <h3 id="does-human--ai-collaboration-work-4">Does Human + AI collaboration work?</h3> <p>The study used <strong>PAID-5 survey scores</strong> for measuring success (a qualitative survey for diabetes, where a <strong>lower score is favorable</strong>). PAID-5 survey scores decreased by a mean of 1.9 points (95% CI, -4.1 to 0.4 points) in the intervention group. However, PAID-5 scores <strong>increased</strong> by a mean of 1.7 points (95% CI, -0.7 to 4.2 points) in the standard of care group, for a mean difference of -3.6 points (95% CI, -6.8 to -0.4 points; P = .03).</p> <h3 id="other-relevant-information-1">Other relevant information</h3> <p>There are some limitations to this study. The study did not compare this rule-based model with other similar insulin titration software to assess whether the voice-based interface benefited participants. Additionally, mean FBG levels worsened in the standard of care group, which could have led to an overestimation of the effect size of the intervention.</p> <h2 id="conventional-versus-artificial-intelligence-assisted-interpretation-of-chest-radiographs-in-patients-with-acute-respiratory-symptoms-in-emergency-department-a-pragmatic-randomized-clinical-trial">Conventional Versus Artificial Intelligence-Assisted Interpretation of Chest Radiographs in Patients With Acute Respiratory Symptoms in Emergency Department: A Pragmatic Randomized Clinical Trial.<d-cite key="Hwang2023-lv"></d-cite></h2> <h5 id="question-does-ai-assistance-predictions-help-radiologists-interpret-chest-radiographs-in-patients-with-acute-respiratory-symptoms-in-ed">Question: Does AI-assistance (predictions) help radiologists interpret chest radiographs in patients with acute respiratory symptoms in ED?</h5> <p>This study explored the effects of having AI model predictions being provided to radiologists for the interpretation of chest radiographs. The study evaluated whether these predictions had any influence in the radiologists‚Äô predictions.</p> <h3 id="study-details-5">Study Details</h3> <p><strong>Study Type:</strong> Pragmatic randomized controlled trial</p> <h4 id="participants-1">Participants</h4> <p>Participants were included in the study if they had met at least one of the criteria:</p> <ul> <li>Patient‚Äôs age &gt;= 19 years and presented to the ED between June 15, 2020, and December 31, 2021</li> <li>Chief complaints are of the following: hills, cough, chest pain, dyspnea, fever, hemoptysis, and sputum</li> <li>Patient was referred for CR</li> </ul> <p>These patients were put into an intervention and a control group in a ratio of 1:1. The intervention group had clinicians review the chest radiograph with an AI model prediction accompanied, while the control group did not. Since the random assignment of which group the patient is put into occured within a few minutes of image aquisition, there was no need for the clinician to request AI consultation, preventing the case of a physician not having access to the AI prediction while viewing the radiograph.</p> <p>One investigator (a thoracic radiologist with 11 years of experience) then reviewed the case 20 days after the emergence department visit (reviewed medical records) in order to define the ground truth.</p> <h3 id="who-were-the-clinicians-4">Who were the clinicians?</h3> <p>The clinicians were duty trainee radiologists in their third year of the residency.</p> <h3 id="model-details-5">Model Details</h3> <p>The model used was the Lunit INSIGHT CXR, version 2.0.2.0 from Lunit Inc. The model analyzes a single frontol chest radiograph to determine the presence of pulmanory nodules, infiltration, and pneumothorax.</p> <h4 id="explainability-method-2">Explainability method</h4> <p>The model used a saliency heatmap to provide explanations for prediction.</p> <h3 id="does-human--ai-collaboration-work-5">Does Human + AI collaboration work?</h3> <p>There was no association between using AI-CAD for CR interpretation and the sensitivity of duty trainee radiologists‚Äô interpretations. (Adjusted odds ratio [OR], 1.02; 95% confidence interval [CI], 0.70‚Äì1.49; P = 0.91)</p> <h2 id="chatgpt-influence-on-medical-decision-making-bias-and-equity-a-randomized-study-of-clinicians-evaluating-clinical-vignettes">ChatGPT Influence on Medical Decision-Making, Bias, and Equity: A Randomized Study of Clinicians Evaluating Clinical Vignettes.<d-cite key="Goh2023-vb"></d-cite></h2> <h5 id="question-can-ai-positively-influence-physicians-in-their-clinical-decision-making-without-introducing-or-exacerbating-demographic-bias">Question: Can AI positively influence physicians in their clinical decision making without introducing or exacerbating demographic bias?</h5> <h3 id="study-details-6">Study Details</h3> <p><strong>Study Type:</strong> Randomized pre-post intervention study</p> <p>Participants reviewed a clinical vignette video of a standarized patient complaining of chest pain. These participants were randomly chosen to either watch a case with a white male or a black female featured. Participants then answered 4 multiple choice questions based on the clinical vignette they watched. These participants were allowed to answer these questions with the option of using any information resource available (e.g., MDCalc, Up-to-Date, PubMed) except for LLMs. Participants then were allowed to review suggestions from ChatGPT+, and subsequently were given the option to change their original answers.</p> <h3 id="who-were-the-clinicians-5">Who were the clinicians?</h3> <p>50 physicians experienced in Family Medicine, Internal Medicine, or Emergency Medicine participated in the study.</p> <h3 id="model-details-6">Model Details</h3> <p>The study used ChatGPT+ (with the GPT-4 model used between May to August 2023).</p> <h3 id="does-human--ai-collaboration-work-6">Does Human + AI collaboration work?</h3> <p>In both cases (vignettes featuring a white male and a black female), accuracy by the clinicians increased by 18% after they reviewed suggestions from ChatGPT+. This shows that physicians are willing to modify their clinical decisions based on suggestions from an LLM. rather than skeptically refusing to be swayed by a computer-generated prediction. Additionally, the results indicate that AI can improve accuracy without introducing or exacerbating existing demographic bias</p> <h2 id="artificial-intelligence-suppression-as-a-strategy-to-mitigate-artificial-intelligence-automation-bias">Artificial intelligence suppression as a strategy to mitigate artificial intelligence automation bias.<d-cite key="Wang2023-hb"></d-cite></h2> <h5 id="question-can-ai-induced-automation-bias-be-mitigated-using-ai">Question: Can AI-induced automation bias be mitigated using AI?</h5> <p>This study proposed and evaluated a method of reducing automation bias. Using the task of diagnosing anterior cruciate ligament (ACL) rupture, a common injury, on magnetic resonance imaging (MRI), the study proposed an AI suppression strategy that retracted AI diagnoses with misleading predictions.</p> <h3 id="study-details-7">Study Details</h3> <p><strong>Study Type:</strong> Case study (on ACL); randomized cross-over design study</p> <p>Clinicians were assigned to either the AI-assisted group or the AI-unassisted group. This was done using block randomization, stratified by physician category. Clinicians asked to diagnose 200 MRIs with or without AI assistance (depending on their group). After a washout period of 14 days, clinicians switched to the other group and diagnosed same 200 MRIs. When diagnosing the MRI, they selected one of the four choices: normal, rupture, mucoid degeneration, and uncertainty.</p> <p>Given these results, the study also proposed an AI suppression strategy, which used an ordinal logistic regression model to predict the correcting and misleading probabilities of the AI model.</p> <h3 id="who-were-the-clinicians-6">Who were the clinicians?</h3> <p>40 clinicians were invited for the study.</p> <h3 id="model-details-7">Model Details</h3> <p>The model incorportated ResNet50 and Siamese algorithms.</p> <h4 id="dataset-2">Dataset</h4> <p>The study collected a training dataset of 8484 magnetic resonance imaging (MRIs). The study used a validation dataset of 2273 prospectively collected knee MRIs.</p> <h4 id="uncertainty-estimation">Uncertainty Estimation</h4> <p>The study divided the AI output range into 9 parts and calculated the accuracy (positive/negative predictive value) of the AI diagnosis for each part using the validation dataset. This accuracy was used as the uncertainty value for a given prediction.</p> <h3 id="model-results-2">Model Results</h3> <p>The model achieved a sensitivity of 90.0%, a specificity of 85.3%, AUC 0.953 when tested on the validation dataset.</p> <h3 id="does-human--ai-collaboration-work-7">Does Human + AI collaboration work?</h3> <p>Application of AI significantly increased the clinicians‚Äô accuracy from 87.2%¬±13.1% to 96.4%¬±1.9%. Clinicians‚Äô sensitivity and specificity also improved with AI assistance.</p> <h4 id="what-happens-when-ai-predicts-incorrectly-1">What happens when AI predicts incorrectly?</h4> <p>45.5% (132/290) of the total mistakes in the AI-assisted round were associated with clinicians being misleaded by AI predictions. However, all clinicians denied that AI predictions in any way misled them to diagnose incorrectly.</p>]]></content><author><name>Karthik S. Vedula</name></author><summary type="html"><![CDATA[A literature review on the role of AI in clinical decision making.]]></summary></entry></feed>